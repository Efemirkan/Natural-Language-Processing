{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa6fdb6e-8076-4f8d-b5fe-e0e2c732388c",
   "metadata": {},
   "source": [
    "# Introduction to spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1144bce-a32b-47b8-a3c5-245c3c430049",
   "metadata": {},
   "source": [
    "At the center of spaCy is the object containing the processing pipeline. We usually call this variable \"nlp\".\r\n",
    "\r\n",
    "For example, to create an English nlp object, you can import spacy and use the spacy.blank method to create a blank English pipeline. You can use the nlp object like a function to analyze text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6064f718-8067-4397-8c7d-8b804e62184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be17fcf4-0861-4616-8f8f-cd0d9a001a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a blank English nlp object\n",
    "nlp = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af7ffbc-fa7a-43ab-9dcd-f0a92d7f467b",
   "metadata": {},
   "source": [
    "When you process a text with the nlp object, spaCy creates a Doc object – short for \"document\". The Doc lets you access information about the text in a structured way, and no information is lost.\r\n",
    "\r\n",
    "The Doc behaves like a normal Python sequence by the way and lets you iterate over its tokens, or get a token by its index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fe99fe-a3f1-473a-bbe2-1015e17a73cc",
   "metadata": {},
   "source": [
    "##### Created by processing a string of text with the nlp object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96ca911a-791b-4f6f-865e-a7c7b7c18d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "world\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Hello world!\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfa21a7-3e4d-4dec-8821-f33567f745d5",
   "metadata": {},
   "source": [
    "Token objects represent the tokens in a document – for example, a word or a punctuation character.\r\n",
    "\r\n",
    "To get a token at a specific position, you can index into the doc.\r\n",
    "\r\n",
    "Token objects also provide various attributes that let you access more information about the tokens. For example, the .text attribute returns the verbatim token text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "983a4fc1-32f4-4642-a77d-1852789ee9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world\n"
     ]
    }
   ],
   "source": [
    "# Index into the Doc to get a single Token\n",
    "token = doc[1]\n",
    "\n",
    "# Get the token text via the .text attribute\n",
    "print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3fcb59-c99e-47e1-a236-c1483ca038b5",
   "metadata": {},
   "source": [
    "A Span object is a slice of the document consisting of one or more tokens. It's only a view of the Doc and doesn't contain any data itself.\n",
    "\n",
    "To create a span, you can use Python's slice notation. For example, 1:3 will create a slice starting from the token at position 1, up to – but not including! – the token at position 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f720798-9273-4138-9aab-e7458d72e0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world!\n"
     ]
    }
   ],
   "source": [
    "# A slice from the Doc is a Span object\n",
    "span = doc[1:3]\n",
    "\n",
    "# Get the span text via the .text attribute\n",
    "print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b06bcd-41ac-4cd4-8a28-b1c64458e344",
   "metadata": {},
   "source": [
    "i is the index of the token within the parent document.\r\n",
    "\r\n",
    "text returns the token text.\r\n",
    "\r\n",
    "is_alpha, is_punct and like_num return boolean values indicating whether the token consists of alphabetic characters, whether it's punctuation or whether it resembles a number. For example, a token \"10\" – one, zero – or the word \"ten\" – T, E, N.\r\n",
    "\r\n",
    "These attributes are also called lexical attributes: they refer to the entry in the vocabulary and don't depend on the token's context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "613a3619-d4bf-469d-b3a6-3cd3a680bf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:    [0, 1, 2, 3, 4]\n",
      "Text:     ['It', 'costs', '$', '5', '.']\n",
      "is_alpha: [True, True, False, False, False]\n",
      "is_punct: [False, False, False, False, True]\n",
      "like_num: [False, False, False, True, False]\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(\"It costs $5.\")\n",
    "print(\"Index:   \", [token.i for token in doc2])\n",
    "print(\"Text:    \", [token.text for token in doc2])\n",
    "\n",
    "print(\"is_alpha:\", [token.is_alpha for token in doc2])\n",
    "print(\"is_punct:\", [token.is_punct for token in doc2])\n",
    "print(\"like_num:\", [token.like_num for token in doc2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a176eb8-5b7e-4d1c-aaf8-e54bb864da92",
   "metadata": {},
   "source": [
    "##### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b75a555-d50f-4677-84c2-3a5348a0cef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sentence.\n"
     ]
    }
   ],
   "source": [
    "# Create the English nlp object\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"This is a sentence.\")\n",
    "\n",
    "# Print the document text\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78c74f86-a314-48f7-ac9f-b94e95cefd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liebe Grüße!\n"
     ]
    }
   ],
   "source": [
    "# Create the German nlp object\n",
    "nlp = spacy.blank(\"de\")\n",
    "\n",
    "# Process a text (this is German for: \"Kind regards!\")\n",
    "doc = nlp(\"Liebe Grüße!\")\n",
    "\n",
    "# Print the document text\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add3b7a2-774a-4545-9513-3eeec8892427",
   "metadata": {},
   "source": [
    "When you call nlp on a string, spaCy first tokenizes the text and creates a document object. In this exercise, you’ll learn more about the Doc, as well as its views Token and Span.\r\n",
    "\r\n",
    "Step 1\r\n",
    "Use spacy.blank to create the English nlp object.\r\n",
    "Process the text and instantiate a Doc object in the variable doc.\r\n",
    "Select the first token of the Doc and print its text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d1320c8-694f-4d35-89c3-cf137885bfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n"
     ]
    }
   ],
   "source": [
    "# create the English nlp object\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\"I like tree kangaroos and narwhals.\")\n",
    "\n",
    "# Select the first token\n",
    "first_token = doc[0]\n",
    "\n",
    "# Print the first token's text\n",
    "print(first_token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef82dced-c244-4205-96f1-ea39cb649c3f",
   "metadata": {},
   "source": [
    "Step 2\r\n",
    "Use spacy.blank to create the English nlp object.\r\n",
    "Process the text and instantiate a Doc object in the variable doc.\r\n",
    "Create a slice of the Doc for the tokens “tree kangaroos” and “tree kangaroos and narwhals”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42836dec-7d9c-498f-969c-4519c0a7bf96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tree kangaroos\n",
      "tree kangaroos and narwhals\n"
     ]
    }
   ],
   "source": [
    "# create the English nlp object\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\"I like tree kangaroos and narwhals.\")\n",
    "\n",
    "# A slice of the Doc for \"tree kangaroos\"\n",
    "tree_kangaroos = doc[2:4]\n",
    "print(tree_kangaroos.text)\n",
    "\n",
    "# A slice of the Doc for \"tree kangaroos and narwhals\" (without the \".\")\n",
    "tree_kangaroos_and_narwhals = doc[2:-1]\n",
    "print(tree_kangaroos_and_narwhals.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743343ef-6de3-4918-a5b0-0b39677d7183",
   "metadata": {},
   "source": [
    "In this example, you’ll use spaCy’s Doc and Token objects, and lexical attributes to find percentages in a text. You’ll be looking for two subsequent tokens: a number and a percent sign.\r\n",
    "\r\n",
    "Use the like_num token attribute to check whether a token in the doc resembles a number.\r\n",
    "Get the token following the current token in the document. The index of the next token in the doc is token.i + 1.\r\n",
    "Check whether the next token’s text attribute is a percent sign ”%“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a49297db-ea3c-4617-8a75-31c61f7df0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage found: 60\n",
      "Percentage found: 4\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\n",
    "    \"In 1990, more than 60% of people in East Asia were in extreme poverty. \"\n",
    "    \"Now less than 4% are.\"\n",
    ")\n",
    "\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if token.like_num:\n",
    "        # Get the next token in the document\n",
    "        next_token = doc[token.i +1]\n",
    "        # Check if the next token's text equals \"%\"\n",
    "        if next_token.text == \"%\":\n",
    "            print(\"Percentage found:\", token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa294b8-94f5-4914-9f1d-345cce5a740d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
